---
title: "NVIDIA Rubin - AI's new fairy tale "
description: "A fun walkthrough inside the NVIDIA Rubin Platform: Six New Chips, One AI Supercomputer"
date: "2026-02-14"
readTime: "1 hr read"
tags: ["Cuda","Gpu","Doca","DPU"]
---

# Understanding the evolution of NVIDIA platforms

A platform is an integrated system comprising CPUs, GPUs, motherboards, power supply units, cooling systems, <b>[NVLink](https://www.nvidia.com/en-in/data-center/nvlink/)</b> switches, and other supporting infrastructure components.
Historically, NVIDIA’s platform development can be categorized into three distinct eras.

## Evolution of NVIDIA platform eras

### Frankenstien era:
In this phase, NVIDIA designed the GPUs, but the broader system stack remained externally integrated. CPUs were primarily sourced from Intel (Xeon) or AMD (EPYC), while server infrastructure, cooling systems, and system integration were handled by OEMs such as Dell, HPE, and Supermicro. NVIDIA did not yet control the full platform stack.
The CPU and GPU were separate devices with independent memory pools and coherency domains. Communication between them occurred over PCIe (Gen3/Gen4), which initially provided sufficient bandwidth for accelerator workloads.

However, as GPU performance scaled exponentially,particularly during the [**Volta**](https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/) -> [**Ampere**](https://www.nvidia.com/en-in/data-center/ampere-architecture/) -> [**early Hopper**](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/) generations the interconnect architecture began to show limitations.

To put the mismatch into perspective:
- PCIe Gen4 x16 bandwidth: ~32 GB/s
- A100 internal memory bandwidth: ~1.5 TB/s
- H100 internal memory bandwidth: ~3 TB/s

The growth curves of GPU compute and memory bandwidth far outpaced the incremental improvements in PCIe. Importantly, the bottleneck was not primarily CPU compute performance. Many AI workloads are GPU-bound. Instead, the critical constraints were:
- Host-device data movement bandwidth (I/O limitations)
- Memory separation between CPU and GPU
- Lack of full memory coherence


As AI workloads became increasingly data-intensive, PCIe bandwidth and latency, along with the host-device separation model, limited overall system efficiency.

### Transition era :
In this phase, NVIDIA began moving towards vertical integration by developing its own CPUs.This phase began with the introduction of the [Grace CPU](https://www.nvidia.com/en-in/data-center/grace-cpu/), NVIDIA’s Arm-based processor, which started the company’s move towards owning the CPU layer of the data center stack.

The first major milestone of this integration was [Grace Hopper (GH200)](https://www.nvidia.com/en-in/data-center/grace-hopper-superchip/), followed by the more advanced **Grace–Blackwell Superchip**. In these systems, the CPU and GPU are tightly integrated within a coherent module, connected via NVLink-C2C, a high-bandwidth, low-latency, cache-coherent interconnect.

This architecture fundamentally differs from the traditional discrete accelerator model:
- Traditional Model:  CPU ↔ PCIe ↔ GPU
- Grace-Based Model:  CPU ⇄ NVLink-C2C ⇄ GPU

The shift delivers a significant higher bandwidth also lowers the latency communication.Here we see cache-coherent memory sharing and a redunction in  host–device separation overhead.This dramatically improves efficiency for large-scale AI and data-intensive workloads.

However, while NVIDIA gained control over the CPU, GPU, interconnect, and system board architecture, it did not yet fully own the rack-level infrastructure. Physical rack components like the power delivery systems,cooling implementations were still largely manufactured and assembled by OEM and ODM partners such as Supermicro, Dell, and HPE. [Blackwell](https://www.nvidia.com/en-in/data-center/technologies/blackwell-architecture/)-based systems represent the maturation of this transition.

### Extreme Co-Design era:

In this phase, NVIDIA moves beyond partial vertical integration and enters what can be described as an extreme co-design model. The company is no longer designing only CPUs and GPUs it is architecting the entire rack-scale system as a unified compute platform.

NVIDIA now create:
- CPU architecture
- GPU architecture
- Interconnect fabric
- Networking topology
- Rack layout
- Power distribution design
- Cooling architecture
- System software stack

This dramatically reduces dependency on traditional OEM-led system assembly. While manufacturing and physical deployment may still involve partners, the architectural blueprint is NVIDIA-defined end-to-end.

A key development in this era is the introduction of next-generation components such as the Vera CPU and the Rubin GPU. These are not designed in isolation but they are engineered together as part of a tightly integrated rack-scale compute fabric.

They no longer optimized components independently because of this extrem co-design. Instead:
- The CPU is designed with GPU memory bandwidth requirements in mind.
- The GPU is designed around networking scale and rack-level power envelopes.
- The interconnect fabric is engineered to match AI workload communication patterns.
- Power and cooling systems are architected alongside silicon performance targets.

In this model, the rack itself behaves more like a single distributed supercomputer rather than a collection of discrete servers.
This represents a structural shift: NVIDIA transitions from being a chip vendor to operating as a full-stack AI infrastructure architect, where silicon, networking, thermals, power delivery, and software are co-optimized as one unified system.

### Conclusion :

```mermaid
flowchart TD
    C["Rubin<br/><b>Rack-scale / data center-scale architecture</b>"]
    B["Blackwell<br/><b>Integrated compute node (CPU + GPU superchip)</b>"]
    A["Ampere / Hopper <br/><b>GPU architecture generationl</b>"]

    A --> B --> C
```




# RUBIN - NVIDIA's latest Gem
## Major Breakthroughs :
![NVIDIA Rubin Platform](/blogs/rubinblog1.png)

### 6th Gen NVLink :
6th Generation NVLink is NVIDIA’s high-speed interconnect enabling direct GPU-to-GPU communication within a node and across systems.
With the Rubin architecture generation, NVLink significantly increases available bandwidth.

Provides up to **3.6 TB/s** aggregate bidirectional bandwidth per GPU across all NVLink connections.
This aggregate bandwidth is critical for minimizing communication bottlenecks in trillion-parameter scale models.

### Vera CPU :
Vera is NVIDIA’s custom ARM-based CPU designed specifically for AI infrastructure.

Key architectural points:
- Built using the ARM instruction set architecture [(ISA)](https://www.arm.com/glossary/isa)
- Implements NVIDIA’s own custom [“Olympus”](https://docs.nvidia.com/olympus-cpu-core-software-optimization-guide-dp12531-001v0-7.pdf) core design
- Not a lightly modified [Neoverse](https://www.arm.com/products/silicon-ip-cpu/neoverse/neoverse-n1) core as companies generally do , this is a ground-up NVIDIA microarchitecture

It is designed to work closely with NVLink-C2C (chip-to-chip interconnect), enabling coherent CPU–GPU memory access and reducing data transfer latency between compute domains.

### Rubin GPU:
The Rubin GPU is NVIDIA’s next-generation AI accelerator optimized for extreme-scale training and inference.

Rubin delivers up to:
- ~50 PFLOPS of AI tensor compute performance using [NVFP4](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/) precision.

#### Precision & Transformer Engine:

Rubin integrates a next-generation Transformer Engine capable of dynamically switching between precision formats based on workload sensitivity:
- NVFP4 → Maximum throughput, lower precision, error-tolerant workloads
- FP8 → Balanced precision and performance
- FP16 → Higher precision for numerically sensitive operations

Adaptive precision switching maximizes computational efficiency by dynamically adjusting numerical precision based on workload requirements, reduces power consumption by using lower precision where full accuracy is unnecessary, maintains numerical stability through intelligent scaling and precision control, and accelerates large-scale AI training by optimizing resource utilization without compromising model performance.


### 3rd Gen Confidential Computing :
Data typically exists in three fundamental states:
- At Rest — Stored on persistent media such as hard drives or SSDs. This state is protected using disk encryption.
- In Transit — When data us moving across networks. Protection is provided by cryptographic protocols such as SSL/TLS (e.g., HTTPS).
- In Use — Actively processed by CPUs or GPUs. Historically, this has been the most vulnerable state because data must be decrypted in system memory to perform computation, potentially exposing it to privileged software such as the operating system, hypervisor, or system administrators.

Traditional systems therefore exhibit a critical security gap - data is protected while stored and transmitted, but not while being processed.

To address this, modern processors implement Trusted Execution Environments [(TEEs)](https://the-scarlet-thread.medium.com/how-trusted-execution-environments-tees-on-nvidia-h100-gpus-secure-ai-without-sacrificing-21b7218c7d17) — a typical vault that isolates , encrypts and stores sensitive code and data from the rest of the system. Here computation can occur without exposing plaintext data to external software, even at high privilege levels.

Evolution of Confidential Computing in Accelerated Systems
- 1st Gen : Early GPU security mechanisms focused on establishing a hardware root of trust rather than full workload isolation. Capabilities included secure boot, firmware authentication, and memory encryption. These features appeared in architectures such as NVIDIA Ampere, while earlier architectures like NVIDIA Volta lacked modern confidential computing capabilities.

- 2nd Gen : The next phase introduced true confidential computing support for GPUs, found in NVIDIA Hopper H100. Key capabilities include GPU operation within CPU confidential virtual machines encrypted PCIe communication, and strong isolation from host software.
In this model, an entire GPU could function as a trusted execution domain, but this was only securely functional for a single GPU.

- 3rd Gen : As discussed earlier, 2nd GEN technology was designed for a single GPU. When data moved from one GPU to another, it had to be decrypted before being transmitted, temporarily exposing . Modern architectures address this limitation by integrating inline hardware encryption directly into the interconnect fabric (NVLink switches). This enables secure communication without exposing plaintext data during transfer.As a result, rack-scale security is achieved. Multiple GPUs, CPUs, and switches can exchange data within a continuously protected domain without leaving the encrypted state. Additionally, cryptographic keys are frequently rotated to maintain a high level of security.


### 2nd GEN RAS Engine :

RAS (Reliability, Availability, Serviceability) is designed to detect faults, contain failures, and maintain continuous operation with minimal or no disruption.
In large training deployments involving thousands of accelerators running for weeks or months traditional maintenance approaches like pausing workloads for checks and repairs are economically and operationally infeasible. Even brief interruptions can waste significant compute time and inur generational loss. But modern systems has emphasized continuous health monitoring and rapid fault containment while workloads remain active.

Evolution of RAS :
- 1st Generation : Platforms based on NVIDIA Hopper H100 introduced systems that could detect and sometimes isolate faults, but often required software intervention or workload disruption.

- 2nd Generation : Next-generation platforms built on NVIDIA Blackwell and extended in NVIDIA Rubin significantly advance RAS capabilities toward autonomous operation.
Key characteristics include continuous health monitoring without pausing workloads, leveraging sideband telemetry that does not interfere with compute data paths. The system observes thousands of hardware signals—such as voltage, temperature, memory bit flops , signal integrity without the involevemnt of GPUs computation. Rather than relying solely on post-failure handling, these systems predicts faults and prepare accordingly before they esclate into critical situations.But this is more of a rapid containment than full predictive prevention.



## Flagship Model :
![NVIDIA Rubin Platform](/blogs/rubinblog3.png)
**VERA RUBIN NVL72**

This model includes :
- 36 VERA CPUs
- 72 RUBIN GPUs
- 9 NVLink Switches

Note -> eflops / exaflops : Earth floating point operations (10<sup>18</sup>)

Peformace :
- 3.6 exaFlops for Inference
- 2.5 exaFlops for Training
- 20.7 TB of [HBM4](https://www.supermicro.com/en/glossary/hbm4) Capacity
- 1.6 PB/s HBM4 Bandwidth
- 260 TB/s Scale up Bandwidth (data transfer between GPUs)
- 54 TB [LPDDR5X](https://in.micron.com/products/memory/dram-components/lpddr5x) (System RAM - CPU)


## Six New Chips in one AI Supercomputer :

![NVIDIA Rubin Platform](/blogs/rubinblog4.png)

- NVIDIA Vera CPU: 88 NVIDIA custom-designed Olympus cores with full Arm-compatibility.
- NVIDIA Rubin GPU: High-performance AI compute with HBM4 and new NVIDIA Transformer Engine.
- NVIDIA NVLink 6 switch: Sixth-generation scale-up fabric delivering 3.6 TB/s GPU-to-GPU bandwidth.
- NVIDIA ConnectX-9: High-throughput, low-latency networking interface (Network card). This is how users communicate with these platforms.
 - NVIDIA BlueField-4 [DPU](https://blogs.nvidia.com/blog/whats-a-dpu-data-processing-unit/): Its a data processing unit with its own Arm-based cores that offloads networking, storage, security, and virtualization tasks from the main CPU and GPU.
- NVIDIA Spectrum-6 Ethernet switch: Scale-out connectivity using co-packaged optics for efficiency and reliability. "Connecting GPUs beyond rack scale".


### VERA CPU :

![NVIDIA Rubin Platform](/blogs/rubinblog5.png)

The transition from NVIDIA Grace to NVIDIA Vera represents a significant step in NVIDIA’s evolution from an accelerator vendor to a full-stack platform designer.

Grace is built on Arm’s Neoverse V2 architecture and integrates 72 high-performance cores. Vera by contrast, introduces 88 custom NVIDIA CPU cores (“Olympus”), signaling a move toward increasingly in-house CPU microarchitecture rather than reliance on licensed Arm core designs.


Vera supports two hardware threads per core [(SMT2)](https://www.yosefk.com/blog/simd-simt-smt-parallelism-in-nvidia-gpus.html), enabling concurrent execution within each core.
- 1 core → 2 simultaneous threads
- 1 CPU → 88 cores
- 1 rack → 36 CPUs

Total potential hardware threads:

<b>36 × 88 × 2 = 6,336 threads</b>


Modern high-performance CPUs often use [chiplet](https://en.wikipedia.org/wiki/Chiplet)-based designs to improve manufacturing yield and scalability. Chiplets can introduce additional latency for inter-die communication, although advanced packaging technologies mitigate this.

Public information does not conclusively specify whether Vera uses a strictly monolithic die package. It is therefore most accurate to state that Vera is engineered to provide uniform high-bandwidth communication across cores.

#### Coherency Fabric

Processors with dozens of cores require an efficient on-chip interconnect to maintain a consistent memory view across threads.

Vera is expected to include a scalable coherency fabric that:
- Connects all cores to shared caches and memory controllers
- Maintains cache coherence across the processor

#### SOCAMM Memory Architecture

- Vera platforms introduce [SOCAMM](https://www.micron.com/products/memory/lpddr-modules/socamm) (Small Outline Compression Attached Memory Module), a new memory design for rack-scale AI systems.
- SOCAMM combines advantages of both soldered memory and traditional DIMMs.
- [LPDDR](https://en.wikipedia.org/wiki/LPDDR) (Low-Power DDR):
    - Soldered directly to the motherboard
    - Very short signal paths → high bandwidth and energy efficiency
    - Not field-replaceable — failure may require replacing the entire board
- [DIMM](https://en.wikipedia.org/wiki/DIMM) (Dual Inline Memory Module)
    - Installed in sockets
    - Easily replaceable and serviceable
    - Longer signal paths → lower bandwidth and higher latency
- SOCAMM — Hybrid Approach
    - SOCAMM modules are mechanically attached (e.g., screwed or compressed) close to the processor rather than permanently soldered.
    - Near-LPDDR performance due to very short signal distances
    - DIMM-like serviceability and replaceability
    - Reduced risk of single-point motherboard failure
    - Physical separation approximately equal to the module thickness

Overall, Vera represents NVIDIA’s push toward highly integrated, CPU-GPU-memory platforms optimized specifically for large-scale AI workloads, with an emphasis on parallelism, bandwidth, and serviceability rather than general-purpose computing alone.



### RUBIN GPU :

![NVIDIA Rubin Platform](/blogs/rubinblog6.png)


The NVIDIA Rubin GPU represents a major leap in architecture design, pushing performance not just through brute force scaling but through deep microarchitectural efficiency.

A single Rubin GPU performace:
- NVFP4 inference ->  ~50 PFlops
- NVFP4 training ->   ~35 PFLops
- HBM4 badnwidth ->   ~22TB/s
- NVLink Bandwidth -> ~3.6 TB/s
- 224 Streaming multiprocessors

Inside each SM, Rubin includes Special Function Units (SFUs), which handle complex mathematical operations that standard arithmetic units are inefficient at computing. These include functions such as sine, cosine, logarithm, exponential, reciprocal, and square root.

A metric like “SFU EX2 ops/clk/SM” indicates how many exponentials (2<sup>x</sup>) calculations a single SM can compute per clock cycle.

Architecturally, each SM is divided into four partitions, and each partition operates semi-independently with its own warp scheduler, execution pipelines, and math units. This design enables extremely high concurrency while minimizing pipeline stalls.A key improvement over the previous generation particularly the Blackwell architecture lies in how Rubin utilizes execution lanes.

Blackwell :
- 16 lanes per SM (4 per partition)
- Each lanes is 32 bit wide (at once it can contain one FP32 number).
- ∴, if we use FP32 on blackwell we can get 16 results per clock tick.
- But if we use FP16 we ge the same 16 results even though FP16 just takes half the width in each slot.

Rubin :
- 32 lanes per SM (8 per partition)
- Same 32 bit width lane but instead of wasting the extra space , in FP16 it just uses [SIMD](https://ftp.cvut.cz/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html) (single instruction multiple data) packing.
- ∴ , Each lane performs two FP16 operations simultaneously. So this is ~2× efficiency for low-precision workloads.
- The efficiency is same for FP32 operations.


GPUs generally serve two fundamentally different computational worlds :
- AI training and inference prioritize speed and throughput and can tolerate reduced numerical precision, which is why formats like FP16, FP8, and FP4 are widely used.
- Scientific computing, on the other hand, demands extremely high precision typically FP64 because even small numerical errors can invalidate results in simulations or physical modeling.

Earlier architectures such as Hopper addressed this by including separate hardware blocks. Tensor Cores for AI and dedicated FP64 units for scientific workloads. However, this created inefficiency because one set of units often sat idle depending on the workload.

Modern designs in Blackwell and Rubin reduce this inefficiency using algorithmic techniques instead of brute-force hardware duplication. One notable method is the [Ozaki](https://arxiv.org/pdf/2508.00441) FP64 algorithm, which decomposes FP64 matrix operations into multiple FP16 operations, executes them on Tensor Cores, and then recombines the results to recover FP64-level accuracy. This approach preserves precision while leveraging the massive throughput of low-precision hardware, significantly improving silicon utilization.

Another critical component is the [Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html), which manages precision scaling for deep learning models. Over successive generations, this subsystem has evolved from simply enabling lower precision to actively optimizing it for real-world model training.

Transformer Engine Evolution
- Gen 1 — Hopper → Introduced FP8 training
- Gen 2 — Blackwell → FP4 support introduced
- Gen 3 — Rubin → Adaptive compression for practical FP4 at massive scale

Overall, Rubin is not just a faster GPU — it represents a shift toward architectures designed explicitly for AI-native computing.

### Interconnect :

NVLink :
- GPU-GPU
- Bi-directional
- Blackwell - ~188 GB/s
- Rubin - ~2600 GB/s

NVLink C2c :
- CPU-GPU
- Bi-directional
- Blackwell - ~900 GB/s
- Rubin - ~1800 GB/s

PCIe Interface :
- host-device
- Bi-directional
- Gen5 x16: ~128 GB/s total
- Gen6 x16: ~256 GB/s total

They use PCIe when users dont buy the superchip (Vera+Rubin) platform instead they get standalone Rubin GPU.

### NVLink 6 Switch :
![NVIDIA Rubin Platform](/blogs/rubinblog7.png)

The 6th-generation NVLink Switch in the NVIDIA Rubin platform is what allows an entire rack of GPUs to behave like a single, tightly coupled AI supercomputer. Instead of GPUs communicating through host CPUs they are interconnected through a high-radix NVLink fabric that provides direct, uniform, low-latency paths between devices.

Within this NVLink domain, the topology is designed to be near-uniform and non-blocking, meaning any GPU can communicate with any other GPU without being constrained by hierarchical bottlenecks. This does not mean contention is impossible , real workloads can still create hotspots — but the fabric is engineered so that bandwidth availability between GPU pairs remains largely consistent even under heavy load.
We can see ~3.6 TB/s bidirectional aggregate NVLink bandwidth per GPU. Note that this bandwidth figure represents the total across all NVLink connections attached to a GPU, not a single link.


Modern large-scale AI training has 2 major bottlenecks :
- Routing data : Mainly for architectures that distribute work dynamically across devices. This becomes extreme in [Mixture-of-Experts](https://huggingface.co/blog/moe) (MoE) models. In these systems, tokens are dispatched to specialized expert networks that may reside on different GPUs, producing highly irregular, sparse, and bursty traffic patterns. Thousands of tokens may need to move simultaneously from one subset of GPUs to another every forward pass.The NVLink 6 Switch addresses this by providing dedicated high-bandwidth routes across the fabric, enabling efficient all-to-all transfers without forcing traffic through slow hierarchical paths. This makes large-scale [expert parallelism](https://nvidia.github.io/TensorRT-LLM/advanced/expert-parallelism.html) feasible and prevents communication from overwhelming compute throughput.

- Synchronizing data : The second major bottleneck is synchronization, particularly during collective operations such as gradient aggregation. In distributed training, GPUs must frequently combine partial results to maintain a consistent model state. Traditional implementations rely on ring or tree algorithms executed by the GPUs themselves, which require multiple communication rounds and generate substantial intermediate traffic.
NVLink 6 introduces hardware-accelerated collectives using technology derived from [SHARP](https://docs.nvidia.com/networking/display/sharpv300) (Scalable Hierarchical Aggregation and Reduction Protocol). Instead of GPUs exchanging and combining data step by step, all participating GPUs can send their values simultaneously into the switch fabric, where reduction operations are performed directly inside the network.
We have dedicated reduction engines inside the switch that can perform upto ~14.4 TFLOPS FP8 peak compute for reduction operations like sum, average, min, and max and the results are broadcasted back to GPUs simultaneously.This significantly reduces latency, bandwidth consumption, and synchronization overhead compared to GPU-driven collectives, especially as system size grows but this is only for collective operations.



### Connect X-9 :

In large distributed AI systems like Mixture-of-Experts (MoE) network traffic arrives in tightly synchronized bursts rather than as a steady stream. At specific phases of training, many GPUs may simultaneously transmit data, producing incast patterns like many-to-one, all-to-all exchanges and microbursts that can overwhelm switch buffers. This sudden surge — often perceived as a “data wall” leads to congestion, queue buildup, retransmissions, and latency spikes that degrade overall cluster efficiency.

The NVIDIA ConnectX-9 [SmartNIC](https://blogs.nvidia.com/blog/what-is-a-smartnic/) mitigates this by controlling how fast data leaves each node and how it reacts to congestion signals from the network.
Instead of allowing all GPUs to inject traffic at full rate at the same instant, ConnectX-9 performs hardware packet pacing and congestion-aware rate control. It spaces transmissions, smooths microbursts, and dynamically throttles sending speed based on real-time feedback (such as [ECN](https://www.juniper.net/documentation/us/en/software/junos/cos/topics/topic-map/explicit-congestion-notification.html)). This prevents switch queues from overflowing while keeping throughput high. In addition, RDMA offload ensures data moves directly between GPU memories with minimal overhead, reducing extra traffic and latency that would otherwise worsen congestion. Hardware encryption offload maintains security without adding processing delays that could compound buffer pressure.

In short, ConnectX-9 solves the “data wall” by regulating traffic at the source: it smooths bursts, adapts transmission rates to network conditions, and enables efficient direct transfers preventing congestion collapse while preserving high utilization.

### BlueField-4 DPU :
![NVIDIA Rubin Platform](/blogs/rubinblog8.png)
BlueField-4 is a Data Processing Unit (DPU) designed to offload infrastructure responsibilities from the host CPU so that CPUs and GPUs can concentrate on application and AI computation. In modern AI servers, a substantial portion of processing time is consumed by networking, storage I/O, security enforcement, virtualization, telemetry, and orchestration tasks that are essential but not part of model execution. BlueField-4 moves this entire infrastructure layer onto a dedicated processor that operates independently of the main compute path.
It has 64 cores of ARM Neoverse V2 [like grace cpu] along with a high-performance RDMA-capable NIC derived from ConnectX-9 technology. This integration enables line-rate networking on the order of ~800 Gb/s.

BlueField-4 also offloads storage networking using language protocols that allow communication between main storage (SSDs) and compute servers:
- [NVMe](https://en.wikipedia.org/wiki/NVM_Express) → Non-Volatile Memory Express, the protocol that modern SSD drives use to communicate.
- [NVMe-oF](https://docs.nvidia.com/doca/sdk/nvme-of---nvm-express-over-fabrics/index.html) → NVMe works only when SSDs are directly plugged in; NVMe over Fabrics sends NVMe commands over network cables.
- [NVMe/TCP](https://nvmexpress.org/specification/tcp-transport-specification/) → Wraps NVMe commands inside a TCP wrapper. However, TCP can be slower because the CPU processes and verifies every packet. BlueField automatically wraps and unwraps TCP, allowing the use of inexpensive Ethernet cables while maintaining high speed.

This DPU is programmable through the NVIDIA [DOCA](https://docs.nvidia.com/doca/sdk/index.html) (Data-Center-on-a-Chip Architecture) software framework, which allows operators to deploy network services, security policies, storage functions, monitoring agents, and infrastructure logic directly on the DPU. BlueField-4 runs its own operating system and forms an independent control and data plane for the server, enabling features such as tenant isolation, zero-trust security enforcement, and out-of-band management.


In large distributed AI clusters, this separation is crucial. The DPU orchestrates data movement between network, storage, and system memory at line rate, ensuring GPUs remain continuously supplied with data while CPUs remain available for application logic. It does not perform model level prediction or AI decisionm aking itself instead, it executes data transfers and infrastructure tasks requested by higher level runtime or orchestration software.



### Spectrum-6 Ethernet Swtich :
![NVIDIA Rubin Platform](/blogs/rubinblog9.png)

Modern AI networking faces two hard limits that become increasingly severe at largescale :
- Physical - Pushing hundreds of gigabits per second through copper traces leads to signal loss, heat generation, and diminishing energy efficiency.
- Power availability - Large AI data centers are often energy-constrained, so networking infrastructure must deliver maximum bandwidth per watt.

This is why achieving maximum efficiency in AI networking is critical.

To scale GPU clusters, two complementary strategies are used :
- Scale-out expands the number of servers within a data center, typically using high-performance Ethernet fabrics such as [Spectrum-X](https://www.nvidia.com/en-us/networking/spectrumx/).
- Scale-across connects geographically separated clusters or data centers, where specialized systems handle long distance latency and reliability. In the NVIDIA networking portfolio, Spectrum-X Ethernet targets intra data center scaling, while [Spectrum-XGS](https://nvidianews.nvidia.com/news/nvidia-introduces-spectrum-xgs-ethernet-to-connect-distributed-data-centers-into-giga-scale-ai-super-factories) addresses inter-site connectivity.

At the silicon level, Spectrum-6 refers to a generation of high-end Ethernet switch [ASICs](https://www.accton.com/Technology-Brief/benefits-programmable-switch-asics/). These chips deliver enormous aggregate throughput on the order of ~100 TB/s using hundreds of high-speed [SerDes](https://www.techtarget.com/searchstorage/definition/SerDes) lanes operating around 200 Gb/s each. This electrical foundation supports both copper links for short distances and optical links for rack-to-rack or row-to-row connectivity.

#### We can see few tehcnologuical changes in Ethernet netrworking :

- Traffic management :
    - Traditional Ethernet -> fabrics often rely on relatively static path selection; once a flow is assigned to a route, congestion on that path can increase latency even if alternative paths exist.
    - Sepctrum X -> AI-optimized Ethernet fabrics instead employ adaptive routing, dynamically spreading traffic across multiple equal cost paths and reacting to real time load conditions. Because packets may traverse different routes, ordering is preserved not by the switch itself but by higher layer mechanisms typically RDMA transport protocols and NIC hardware which reassemble streams correctly at the destination.

- Physical connectivity :
    - Pluggable transceievers -> Conventional pluggable optical modules require high speed electrical signals to travel across the switch motherboard before conversion to light, increasing power consumption and thermal load.
    - CPO (Co packaged optics) -> This move the optical engines directly into the switch package using silicon photonics, drastically shortening electrical paths. The heat generating laser sources are placed in a separate module, improving cooling efficiency while reducing signal loss and power requirements.

- Handling bursty AI traffic :
    - Tranditioal -> Workloads such as Mixture-of-Experts training causes many GPUs to send data at once (incast), switch buffers quickly overflow leading to packet drops, retransmissions, sharp latency spikes and degraded throughput because congestion is handled only after loss occurs.
    - Modern AI networking -> Purpose-built fabrics anticipate these burst patterns and use congestion-aware mechanisms (e.g., [ECN](https://www.juniper.net/documentation/us/en/software/junos/cos/topics/topic-map/explicit-congestion-notification.html), [PFC](https://docs.nvidia.com/networking/display/onyxv3103004/priority+flow+control+(pfc)), telemetry-driven feedback) to signal endpoints before buffers overflow, allowing NICs to throttle transmission rates proactively and maintain stable, high throughput communication without packet loss.

Overall, Spectrum-6 class switches serve as the high capacity foundation of AI Ethernet fabrics, while platforms built on top of them provide the system-level intelligence required for large scale training clusters.