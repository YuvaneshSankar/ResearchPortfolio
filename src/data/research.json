[
  {
    "id": "frugalsot-research",
    "title": "FrugalSOT: Adaptive Model Selection for Edge Computing",
    "description": "A resource-aware model selection architecture for on-device NLP inference that dynamically chooses optimal LLMs based on request complexity. Achieved 21.34% reduction in processing time while maintaining 0.844 relevance score on Raspberry Pi 5.",
    "github": "https://github.com/HyperKuvid-Labs/FrugalSOT",
    "demo": "https://frugalsot.vercel.app/",
    "paper": "https://frugalsot.vercel.app/FrugalSOT.pdf"
  },

    {
    "id": "specquant-research",
    "title": "SpecQuant: Speculative Decoding with Multi-Parent Quantization for Adaptive LLM Inference",
    "description": "Adaptive LLM inference framework that classifies prompt complexity and dynamically selects quantized draft models (Q4/Q8) for speculative decoding with an FP16 target, achieving 35-43% faster inference on consumer hardware without retraining.",
    "github": "https://github.com/HyperKuvid-Labs/SpecQuant",
    "demo": "https://specquant.vercel.app/",
    "paper": "https://specquant.vercel.app/SpecQuant.pdf"
  }
]

