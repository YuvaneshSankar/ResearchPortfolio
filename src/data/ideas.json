[
  {
    "id": "1",
    "title": "Energy-Based Token Throttling for LLMs with RLHF",
    "description": "An energy-aware RLHF framework balances LLM quality and hardware efficiency by adjusting speculative decoding to keep energy use in the 95–98% sweet spot.",
    "tags": ["RLHF", "Energy-efficiency", "SGLang-Eagle3"],
    "createdAt": "2025-09-04T00:00:00.000Z",
    "mainIdea": "Develop an energy-aware reinforcement learning framework that trains LLMs using RLHF with dual reward signals: traditional optimum rewards for model quality and energy-based rewards for hardware efficiency. The core innovation involves maintaining energy utilization within a tight 95-98% range by implementing negative MSE-based rewards when energy consumption drops below 95% (underutilization) or exceeds 98% (crash risk). The system continuously monitors comprehensive hardware metrics including CPU core temperatures, GPU temperatures, fan speeds, and remaining battery charge to create a real-time energy state vector. This energy state directly controls the number of parallel tokens generated through speculative decoding techniques, specifically leveraging Medusa or similar parallel token generation methods. When energy levels approach the upper threshold (98%), the system reduces parallel token count to prevent thermal throttling and potential crashes. Conversely, when energy drops below 95%, it increases parallel token generation to maximize resource utilization. The LLM itself serves as the policy network, enhanced with speculative decoding capabilities that adapt token generation parallelism based on energy constraints. The value network provides multi-objective rewards combining answer correctness (through RLVR-based hard rewards or perplexity-based soft rewards) with energy efficiency metrics. This creates a dynamic training environment where the model learns to self-regulate computational intensity while maintaining high-quality outputs. The ultimate goal is to discover the optimal energy-performance sweet spot - analogous to AlphaGo's Move 37 - that maximizes training throughput without causing system instability, crashes, or thermal damage that currently plague intensive LLM training on resource-constrained hardware.",
    "workDone": "Initial conceptualization phase - no implementation or literature review completed yet. The core hypothesis is that dynamic token throttling based on energy constraints can achieve optimal resource utilization while maintaining model performance through adaptive speculative decoding strategies."
  },

    {
    "id": "2",
    "title": "Hybrid Reinforcement Learning and Genetic Algorithm for F1 Front Wing Aerodynamic Optimization",
    "description": "A hybrid GA-RL framework optimizes F1 front wings by combining GA’s global exploration with RL’s adaptive refinement. It maximizes downforce and efficiency while minimizing drag under FIA and manufacturing constraints.",
    "tags": ["Reinforcement-learning", "Genetic-algorithm","aerodynamics"],
    "createdAt": "2025-07-28T00:00:00.000Z",
    "mainIdea": "This research proposes a novel hybrid optimization approach that synergistically combines the global exploration capabilities of genetic algorithms (GA) with the adaptive local refinement abilities of reinforcement learning (RL) for aerodynamic optimization of Formula 1 (F1) front wings. The primary goal is to optimize the multi-element front wing design by maximizing downforce and aerodynamic efficiency while minimizing drag and maintaining compliance with FIA regulations and manufacturing constraints. The system employs a hierarchical parameterization of wing geometry including main elements, flaps, and endplates, represented via Bezier curves for smooth, manufacturable surfaces with curvature continuity enforcement. The RL component uses policy-gradient methods with separate policy and value networks that suggest incremental geometry modifications based on aerodynamic performance feedback. The GA provides global search through operations tailored for aerodynamic coherence, such as crossover strategies preserving wing structure and flap system integrity, and constraint-aware mutation. The hybrid architecture integrates GA and RL hierarchically, where GA explores broadly and RL refines promising designs with curriculum-based reward functions balancing performance, constraint satisfaction, and design novelty. The evaluation relies on high-fidelity CFD simulations with smart scheduling and surrogate modeling to reduce computational cost. This synergy leverages RL's adaptive learning with GA's robust exploration to overcome local optima and achieve efficient, compliant, and manufacturable wing designs optimized for diverse F1 racing conditions.",
    "workDone": "The project has developed a comprehensive hybrid optimization framework integrating genetic algorithms with neural network-based policy learning, including F1-specific parameter initialization, constraint handling, and multi-objective fitness evaluation. Neural network architecture with policy-value heads leveraging GELU activations and AdamW optimization has been implemented with curriculum learning. CFD pipeline integration with smart scheduling and surrogate models has reduced computational overhead by 40%. Preliminary experiments demonstrated successful hybrid operation, improving fitness scores and constraint compliance across generations. System validation includes end-to-end pipeline integration, configuration management, and data versioning ensuring robust large-scale optimization capability."
  },

  {
    "id": "3",
    "title": "Adaptive Model Optimization Framework for Overcoming Catastrophic Forgetting",
    "description": "An adaptive continual learning router mitigates catastrophic forgetting by dynamically selecting methods like PackNet, EWC, or Progressive Nets based on data shifts and model constraints. It ensures flexible, context-aware optimization that balances stability and plasticity across evolving datasets.",
    "tags": ["Reinforcement-learning", "Genetic-algorithm","aerodynamics"],
    "createdAt": "2025-08-15T00:00:00.000Z",
    "mainIdea": "This idea proposes an adaptive optimization framework designed to mitigate catastrophic forgetting in neural network training by dynamically selecting and applying state-of-the-art continual learning techniques. The framework operates like a router that takes key hyperparameters such as the original model to optimize, incoming new data characteristics, and the model architecture as inputs. Based on these inputs, it intelligently routes the training process through different continual learning methods including PackNet (network pruning and repurposing), SupSup (Supermasks in Superposition using binary masks), Elastic Weight Consolidation (Fisher information matrix-based weight importance), Synaptic Intelligence (contribution-based weight classification), Memory Aware Synapses (gradient-based importance estimation), and Progressive Neural Networks (adding new columns for new tasks). The system analyzes the incoming data distribution shift, model capacity constraints, and architectural compatibility to determine the most suitable continual learning approach for each training scenario. This modular and configurable approach allows seamless adaptation to diverse training scenarios and data distributions, ensuring efficient knowledge retention while incorporating new information. The key advantage is flexible, context-aware model optimization that balances stability and plasticity, minimizing performance degradation over successive training iterations on evolving datasets. The router-like configuration enables practitioners to leverage the best aspects of each method based on their specific use case requirements.",
    "workDone": "Conceptualization of a flexible routing optimization framework integrating major continual learning methodologies. Surveyed state-of-the-art approaches to catastrophic forgetting including PackNet, SupSup, EWC, SI, MAS, and PNN, analyzing their strengths and application contexts. Designed prototype architecture for adaptive routing based on model, data, and architecture parameters. Initial framework design completed with decision logic for method selection based on hyperparameter analysis."
  },

    {
    "id": "4",
    "title": "FrugalSOT: Adaptive Resource-Aware Model Selection for On-Device NLP Inference",
    "description": "FrugalSOT is a resource-aware framework for on-device NLP that adaptively selects between lightweight and larger LMs based on input complexity and relevance thresholds. It cuts computation and energy use on constrained hardware like Raspberry Pi 5 while preserving output quality.",
    "tags": ["adaptive-model-selection", "edge-computing","NLP"],
    "createdAt": "2024-09-24T00:00:00.000Z",
    "mainIdea": "FrugalSOT proposes a resource-aware adaptive model selection framework for on-device NLP inference, optimized for constrained hardware like Raspberry Pi 5. The system dynamically assesses the complexity of each input using lightweight feature extraction including prompt length, named entity density, and syntactic complexity to select the most suitable language model from a hierarchy of models with varying sizes and capabilities. By starting inference on simpler, less resource-intensive models (TinyLlama 1.1B, TinyDolphin 2.7B) and escalating to more sophisticated models (Gemma2 2b, Phi 2.7b) only when output relevance falls below adaptive thresholds, FrugalSOT minimizes computational load, reduces latency, and preserves energy without significant loss in output quality. The system employs cosine similarity-based relevance evaluation using the all-MiniLM-L6-v2 embedding model to determine when escalation is necessary. The framework continuously refines its relevance thresholds using a low-pass adaptive filter with exponential moving averages (α = 0.2) to respond effectively to changing input patterns, user preferences, and workload variations. This ensures sustained high-quality NLP performance on resource-limited edge devices while maintaining system stability and preventing threshold oscillations. This approach bridges the gap between high-quality NLP inference and the constraints of edge hardware by managing computation efficiently through hierarchical model cascading, adaptive thresholding, and intelligent fallback mechanisms, achieving up to 21.34% reduction in processing time with minimal quality degradation.",
    "workDone": "Developed and validated FrugalSOT with comprehensive prompt complexity classification system using weighted scoring (length, NER, syntactic complexity). Implemented hierarchical model selection tailored to available device memory (<8GB vs >8GB configurations) and query complexity levels. Created dynamic relevance evaluation using cosine similarity with empirically-derived thresholds (0.4441 for low, 0.6537 for medium, 0.6934 for high complexity). Successfully deployed and tested on Raspberry Pi 5, demonstrating 21.34% reduction in inference time while maintaining 0.844 relevance score comparable to best individual models. Implemented adaptive threshold algorithms with exponential moving averages for stable system tuning."
  }
]